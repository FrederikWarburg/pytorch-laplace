{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11946ddf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
    "\n",
    "import nnj\n",
    "from nnj.utils import convert_to_nnj\n",
    "from pytorch_laplace.hessian.contrastive import ArccosHessianCalculator\n",
    "from pytorch_laplace.laplace.diag import DiagLaplace\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from plotting_fun import plot_latent, plot_reconstruction_with_latent, plot_std, plot_fancy_latent, plot_attention, plot_training\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-metric-learning in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from pytorch-metric-learning) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from pytorch-metric-learning) (1.2.2)\n",
      "Requirement already satisfied: tqdm in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from pytorch-metric-learning) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from pytorch-metric-learning) (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.6.3)\n",
      "Requirement already satisfied: sympy in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from torch>=1.6.0->pytorch-metric-learning) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from torch>=1.6.0->pytorch-metric-learning) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from scikit-learn->pytorch-metric-learning) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from scikit-learn->pytorch-metric-learning) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.2.1)\n",
      "Collecting faiss-gpu\n",
      "  Using cached faiss-gpu-1.7.1.post2.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/17/76/47d0cc8161f4bf988583a2839bb1e56baf09d6b80cfa472b9eba4d5f543b/faiss-gpu-1.7.1.post2.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
      "  Using cached faiss-gpu-1.7.1.post1.tar.gz (41 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/39/8d/b62bc92c8dd4b2a99d4a06b8804280f6445748b6d698eabb037e111080c7/faiss-gpu-1.7.1.post1.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
      "  Using cached faiss-gpu-1.7.1.tar.gz (40 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/51/85/7a7490dbecaea9272953b88e236a45fe8c47571a069bc28b352f0b224ea3/faiss-gpu-1.7.1.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
      "  Using cached faiss-gpu-1.7.0.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/63/15/289ecf5d23f209c4c6f2f5f4db1d2b4a2be22d1fc49619354363e9367c19/faiss-gpu-1.7.0.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
      "  Using cached faiss-gpu-1.6.5.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/9c/27/3477c856ea3d678619c33ae72f89ede4fbe08e9c5ba3b89a5feb3d9938b0/faiss-gpu-1.6.5.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
      "  Using cached faiss-gpu-1.6.4.post2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz (from https://pypi.org/simple/faiss-gpu/)\u001b[0m: \u001b[33mRequested faiss-cpu from https://files.pythonhosted.org/packages/7d/00/b3aaad408a44e4f5d87ebfcd75d0b14eeaed9fe9bc7a9f5e185ff1d503d6/faiss-gpu-1.6.4.post2.tar.gz has inconsistent name: expected 'faiss-gpu', but metadata has 'faiss-cpu'\u001b[0m\n",
      "  Using cached faiss-gpu-1.6.4.tar.gz (3.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages/setuptools/__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/tp/k8_xb41s1hqc7d25x2j_3bbw0000gn/T/pip-pip-egg-info-y7less_5/faiss_cpu.egg-info\n",
      "  \u001b[31m   \u001b[0m writing /private/var/folders/tp/k8_xb41s1hqc7d25x2j_3bbw0000gn/T/pip-pip-egg-info-y7less_5/faiss_cpu.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to /private/var/folders/tp/k8_xb41s1hqc7d25x2j_3bbw0000gn/T/pip-pip-egg-info-y7less_5/faiss_cpu.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to /private/var/folders/tp/k8_xb41s1hqc7d25x2j_3bbw0000gn/T/pip-pip-egg-info-y7less_5/faiss_cpu.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m writing manifest file '/private/var/folders/tp/k8_xb41s1hqc7d25x2j_3bbw0000gn/T/pip-pip-egg-info-y7less_5/faiss_cpu.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m error: package directory '/private/var/folders/tp/k8_xb41s1hqc7d25x2j_3bbw0000gn/T/pip-install-mk5ymcg6/faiss-gpu_9f67d667ce834bb9a3f9c84fbb6c859d/faiss/faiss/python' does not exist\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-metric-learning\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 0: Loss = 0.9724591374397278, Number of mined triplets = 560992\n",
      "Epoch 1 Iteration 20: Loss = 0.8178108334541321, Number of mined triplets = 147869\n",
      "Epoch 1 Iteration 40: Loss = 0.8217043280601501, Number of mined triplets = 156738\n",
      "Epoch 1 Iteration 60: Loss = 0.8481734991073608, Number of mined triplets = 97370\n",
      "Epoch 1 Iteration 80: Loss = 0.7788268327713013, Number of mined triplets = 77356\n",
      "Epoch 1 Iteration 100: Loss = 0.8116377592086792, Number of mined triplets = 92069\n",
      "Epoch 1 Iteration 120: Loss = 0.8057810664176941, Number of mined triplets = 86491\n",
      "Epoch 1 Iteration 140: Loss = 0.794864296913147, Number of mined triplets = 55119\n",
      "Epoch 1 Iteration 160: Loss = 0.7977604269981384, Number of mined triplets = 47115\n",
      "Epoch 1 Iteration 180: Loss = 0.8576111197471619, Number of mined triplets = 96745\n",
      "Epoch 1 Iteration 200: Loss = 0.7837868928909302, Number of mined triplets = 43934\n",
      "Epoch 1 Iteration 220: Loss = 0.7627668976783752, Number of mined triplets = 37053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:16<00:00, 115.49it/s]\n",
      "100%|██████████| 313/313 [00:03<00:00, 88.85it/s] \n",
      "/Users/warburg/miniconda3/envs/laplace/lib/python3.8/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.9822\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                \"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(\n",
    "                    epoch, batch_idx, loss, mining_func.num_triplets\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, test_labels, train_embeddings, train_labels, False\n",
    "    )\n",
    "    print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset1 = datasets.MNIST(\"../../../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\"../../../data\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset1, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=batch_size)\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "model = nnj.Sequential(\n",
    "    nnj.Conv2d(1, 32, 3, 1),\n",
    "    nnj.Tanh(),\n",
    "    nnj.Conv2d(32, 64, 3, 1),\n",
    "    nnj.Tanh(),\n",
    "    nnj.MaxPool2d(2),\n",
    "    nnj.Flatten(),\n",
    "    nnj.Linear(9216, 128),\n",
    "    nnj.L2Norm(),\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "### pytorch-metric-learning stuff ###\n",
    "distance = distances.LpDistance()\n",
    "loss_func = losses.ContrastiveLoss(\n",
    "    pos_margin=0, neg_margin=0.8, distance=distance\n",
    ")\n",
    "\n",
    "mining_func = miners.TripletMarginMiner(\n",
    "    margin=0.2, distance=distance, type_of_triplets=\"semihard\"\n",
    ")\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "### pytorch-metric-learning stuff ###\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    test(dataset1, dataset2, model, accuracy_calculator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace posthoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_calculator = ArccosHessianCalculator(\n",
    "    wrt=\"weight\",\n",
    "    shape=\"diagonal\",\n",
    "    speed=\"half\",\n",
    "    method=\"fix\",\n",
    ")\n",
    "\n",
    "# if arccos, then remove normalization layer from model\n",
    "model_nnj = convert_to_nnj(model[:-1])\n",
    "\n",
    "laplace = DiagLaplace()\n",
    "_prior_prec = 0.01 # weight of the l2 regularizer\n",
    "_prior_prec_multiplier = 10000 #prior optimization (made a posteriori by the Laplace Redux guys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "max_pairs = 100\n",
    "\n",
    "mean = parameters_to_vector(model.parameters())\n",
    "prior_hessian = _prior_prec_multiplier *_prior_prec * torch.ones_like(mean)\n",
    "hessian = 0\n",
    "for batch in tqdm(train_loader):\n",
    "    x, y = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(x)\n",
    "        indices_tuple = mining_func(embeddings, y)\n",
    "\n",
    "        # randomly choose 5000 pairs if more than 5000 pairs available.\n",
    "        if len(indices_tuple[0]) > max_pairs:\n",
    "            idx = torch.randperm(indices_tuple[0].size(0))[: max_pairs]\n",
    "            indices_tuple = (\n",
    "                indices_tuple[0][idx],\n",
    "                indices_tuple[1][idx],\n",
    "                indices_tuple[2][idx],\n",
    "            )\n",
    "\n",
    "        hessian += hessian_calculator.compute_hessian(x, model_nnj, indices_tuple)\n",
    "\n",
    "precision = prior_hessian + hessian\n",
    "std_deviation = 1.0 / precision.sqrt()\n",
    "\n",
    "plot_attention(std_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indices_tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m indices_tuple\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indices_tuple' is not defined"
     ]
    }
   ],
   "source": [
    "indices_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
