{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
    "\n",
    "import nnj\n",
    "from pytorch_laplace.hessian.contrastive import ArccosHessianCalculator\n",
    "from pytorch_laplace.laplace.diag import DiagLaplace\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from plotting_fun import plot_latent, plot_reconstruction_with_latent, plot_std, plot_fancy_latent, plot_attention, plot_training\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-metric-learning\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                \"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(\n",
    "                    epoch, batch_idx, loss, mining_func.num_triplets\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, test_labels, train_embeddings, train_labels, False\n",
    "    )\n",
    "    print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset1 = datasets.MNIST(\"../../../data\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\"../../../data\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset1, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=batch_size)\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "model = nnj.Sequential(\n",
    "    nnj.Conv2d(1, 32, 3, 1),\n",
    "    nnj.Tanh(),\n",
    "    nnj.Conv2d(32, 64, 3, 1),\n",
    "    nnj.Tanh(),\n",
    "    nnj.MaxPool2d(2),\n",
    "    nnj.Flatten(),\n",
    "    nnj.Linear(9216, 128),\n",
    "    nnj.L2Norm(),\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "### pytorch-metric-learning stuff ###\n",
    "distance = distances.LpDistance()\n",
    "loss_func = losses.ContrastiveLoss(\n",
    "    pos_margin=0, neg_margin=0.8, distance=distance\n",
    ")\n",
    "\n",
    "mining_func = miners.TripletMarginMiner(\n",
    "    margin=0.2, distance=distance, type_of_triplets=\"semihard\"\n",
    ")\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "### pytorch-metric-learning stuff ###\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    test(dataset1, dataset2, model, accuracy_calculator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace posthoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model parameters\n",
    "\n",
    "\n",
    "hessian_calculator = ArccosHessianCalculator(\n",
    "    wrt=\"weight\",\n",
    "    shape=\"diagonal\",\n",
    "    speed=\"half\",\n",
    "    method=\"fix\",\n",
    ")\n",
    "\n",
    "# if arccos, then remove normalization layer from model\n",
    "model = nnj.convert_to_nnj(model[:-1])\n",
    "\n",
    "laplace = DiagLaplace()\n",
    "_prior_prec = 0.01 # weight of the l2 regularizer\n",
    "_prior_prec_multiplier = 10000 #prior optimization (made a posteriori by the Laplace Redux guys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = parameters_to_vector(model.parameters())\n",
    "prior_hessian = _prior_prec_multiplier *_prior_prec * torch.ones_like(mean)\n",
    "hessian = 0\n",
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "\n",
    "    embeddings = model(x)\n",
    "    tuple_indices = mining_func(embeddings, y)\n",
    "    hessian += hessian_calculator.compute_hessian(x, model)\n",
    "\n",
    "precision = prior_hessian + hessian\n",
    "std_deviation = 1.0 / precision.sqrt()\n",
    "\n",
    "plot_attention(std_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
