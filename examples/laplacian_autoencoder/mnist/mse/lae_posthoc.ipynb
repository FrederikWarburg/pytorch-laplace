{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
    "\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "from nnj import nnj\n",
    "from pytorch_laplace.hessian.mse import MSEHessianCalculator\n",
    "from pytorch_laplace.laplace.diag import DiagLaplace\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "from plotting_fun import (\n",
    "    plot_latent,\n",
    "    plot_reconstruction_with_latent,\n",
    "    plot_std,\n",
    "    plot_fancy_latent,\n",
    "    plot_attention,\n",
    "    plot_training,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boring dataset stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.MNIST(\n",
    "    \"../../../../data/\", train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "dataset_test = torchvision.datasets.MNIST(\n",
    "    \"../../../../data/\", train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "def get_batch(dataset, batch_size):\n",
    "    datas, targets = [], []\n",
    "    for i in range(batch_size):\n",
    "        img, t = dataset.__getitem__(i)\n",
    "        target = torch.zeros(10)\n",
    "        target[t] = 1  # one-hot-encoded targets\n",
    "        datas.append(img)\n",
    "        targets.append(target)\n",
    "    datas = torch.stack(datas, dim=0)\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "    return datas, targets\n",
    "\n",
    "\n",
    "imgs, labels = get_batch(dataset, 100)  # train with 100 images\n",
    "imgs_test, labels_test = get_batch(dataset_test, 100)  # test with 100 images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_latent_size = 2\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    encoder = nnj.Sequential(\n",
    "        nnj.Flatten(),\n",
    "        nnj.Linear(28 * 28, 50),\n",
    "        nnj.Tanh(),\n",
    "        nnj.Linear(50, _latent_size),\n",
    "        nnj.L2Norm(),\n",
    "        add_hooks=True,\n",
    "    )\n",
    "\n",
    "    decoder = nnj.Sequential(\n",
    "        nnj.Flatten(),\n",
    "        nnj.Linear(_latent_size, 50),\n",
    "        nnj.Tanh(),\n",
    "        nnj.Linear(50, 28 * 28),\n",
    "        nnj.Reshape(1, 28, 28),\n",
    "        add_hooks=True,\n",
    "    )\n",
    "\n",
    "    model = nnj.Sequential(encoder, decoder, add_hooks=True)\n",
    "\n",
    "    return encoder, decoder, model\n",
    "\n",
    "\n",
    "encoder, decoder, model = get_model()\n",
    "\n",
    "encoder_size = len(parameters_to_vector(encoder.parameters()))\n",
    "decoder_size = len(parameters_to_vector(decoder.parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Post-Hoc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train standard gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_learning_rate = 0.05\n",
    "_epoch_num = 5000\n",
    "_prior_prec = 0.01  # weight of the l2 regularizer\n",
    "_prior_prec_multiplier = 10000  # prior optimization (made a posteriori by the Laplace Redux guys)\n",
    "\n",
    "encoder, decoder, model = get_model()\n",
    "mse = MSEHessianCalculator(wrt=\"weight\", shape=\"diagonal\", speed=\"half\")\n",
    "sampler = DiagLaplace()\n",
    "\n",
    "# stantard train\n",
    "losses, losses_test, priors = [], [], []\n",
    "with torch.no_grad():\n",
    "    for epoch in tqdm(range(_epoch_num)):\n",
    "        # (Mean Squared Error + l2 reg)\n",
    "        # loss =\n",
    "        #  = - log p(y,theta)\n",
    "        #  = - log p(y|theta) - log p(theta)\n",
    "        #  = - log_gaussian - log_prior\n",
    "\n",
    "        parameters = parameters_to_vector(model.parameters())\n",
    "\n",
    "        # 0-order\n",
    "        log_gaussian = mse.compute_loss(imgs, imgs, model).detach().numpy()\n",
    "        log_gaussian_test = mse.compute_loss(imgs_test, imgs_test, model).detach().numpy()\n",
    "        log_prior = 0.5 * _prior_prec * torch.sum(parameters**2).detach().numpy()\n",
    "        losses.append(log_gaussian + log_prior)\n",
    "        losses_test.append(log_gaussian_test + log_prior)\n",
    "        priors.append(log_prior)\n",
    "\n",
    "        # 1-order\n",
    "        gradient_log_gaussian = mse.compute_gradient(imgs, imgs, model)\n",
    "        gradient_log_prior = _prior_prec * parameters\n",
    "        gradient = gradient_log_gaussian + gradient_log_prior\n",
    "\n",
    "        # gradient step\n",
    "        parameters -= _learning_rate * gradient\n",
    "\n",
    "        vector_to_parameters(parameters, model.parameters())\n",
    "\n",
    "plot_training(losses, losses_test, priors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute posterior on weights\n",
    "(and try to visualize it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = parameters_to_vector(model.parameters())\n",
    "prior_hessian = _prior_prec_multiplier * _prior_prec * torch.ones_like(mean)\n",
    "precision = prior_hessian + mse.compute_hessian(imgs, model)\n",
    "\n",
    "std_deviation = 1.0 / precision.sqrt()\n",
    "\n",
    "plot_std(std_deviation, encoder_size, decoder_size)\n",
    "plot_attention(std_deviation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fancy_latent(encoder, imgs)\n",
    "\n",
    "encoder_mean = mean[:encoder_size]\n",
    "encoder_std = std_deviation[:encoder_size]\n",
    "plot_latent(encoder, imgs, posterior=(encoder_mean, encoder_std), scale_radius=1, n_sample=1000)\n",
    "\n",
    "for data_idx in range(10):  # only show the first 10 images\n",
    "    plot_reconstruction_with_latent(\n",
    "        model, imgs, posterior=(mean, std_deviation), data_idx=data_idx, scale_radius=1, n_sample=1000\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_mean = mean[:encoder_size]\n",
    "encoder_std = std_deviation[:encoder_size]\n",
    "\n",
    "plot_latent(encoder, imgs_test, posterior=(encoder_mean, encoder_std), scale_radius=1, n_sample=1000)\n",
    "plot_fancy_latent(encoder, imgs_test)\n",
    "\n",
    "for data_idx in range(10):  # only show the first 10 images\n",
    "    plot_reconstruction_with_latent(\n",
    "        model, imgs_test, posterior=(mean, std_deviation), data_idx=data_idx, scale_radius=1, n_sample=1000\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample only the encoder (fix the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation[-decoder_size:] = torch.zeros(decoder_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_idx in range(5):  # only show the first 5 images\n",
    "    plot_reconstruction_with_latent(\n",
    "        model, imgs, posterior=(mean, std_deviation), data_idx=data_idx, scale_radius=1, n_sample=1000\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_idx in range(5):  # only show the first 5 images\n",
    "    plot_reconstruction_with_latent(\n",
    "        model, imgs_test, posterior=(mean, std_deviation), data_idx=data_idx, scale_radius=1, n_sample=1000\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gauss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6c099567d37d7e8a434c28d3f0abbe2f74c3f11df2455f666970f25a5926fa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
