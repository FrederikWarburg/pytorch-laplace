{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.utils import vector_to_parameters, parameters_to_vector\n",
    "\n",
    "import nnj\n",
    "from pytorch_laplace.hessian.bce import BCEHessianCalculator\n",
    "from pytorch_laplace.laplace.diag import DiagLaplace\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "from plotting_fun import plot_latent, plot_reconstruction_with_latent, plot_std, plot_fancy_latent, plot_attention, plot_training\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boring dataset stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.MNIST('../../../../data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "dataset_test = torchvision.datasets.MNIST('../../../../data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "def get_batch(dataset, batch_size):\n",
    "    datas, targets = [], []\n",
    "    for i in range(batch_size):\n",
    "        img, t = dataset.__getitem__(i)\n",
    "        target = torch.zeros(10)\n",
    "        target[t] = 1 #one-hot-encoded targets\n",
    "        datas.append(img)\n",
    "        targets.append(target)\n",
    "    datas = torch.stack(datas, dim=0)\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "    return datas, targets\n",
    "\n",
    "imgs, labels = get_batch(dataset, 100) #train with 100 images\n",
    "imgs_test, labels_test = get_batch(dataset_test, 100) #test with 100 images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_latent_size = 2\n",
    "\n",
    "def get_model():\n",
    "        encoder = nnj.Sequential(\n",
    "                nnj.Flatten(),\n",
    "                nnj.Linear(28*28, 50),\n",
    "                nnj.Tanh(),\n",
    "                nnj.Linear(50, _latent_size),\n",
    "                nnj.L2Norm(),\n",
    "                add_hooks = True\n",
    "        )\n",
    "\n",
    "        decoder = nnj.Sequential(\n",
    "                nnj.Flatten(),\n",
    "                nnj.Linear(_latent_size, 50),\n",
    "                nnj.Tanh(),\n",
    "                nnj.Linear(50, 28*28),\n",
    "                nnj.Reshape(1, 28, 28),\n",
    "                add_hooks = True\n",
    "        )\n",
    "\n",
    "        model = nnj.Sequential(\n",
    "                encoder,\n",
    "                decoder,\n",
    "                add_hooks = True\n",
    "        )\n",
    "        \n",
    "        return encoder, decoder, model\n",
    "encoder, decoder, model = get_model()\n",
    "\n",
    "encoder_size = len(parameters_to_vector(encoder.parameters()))\n",
    "decoder_size = len(parameters_to_vector(decoder.parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_epoch_num = 1000\n",
    "_learning_rate = 0.05\n",
    "_memory_factor = 0.005\n",
    "_prior_prec = 0.01  # weight of the l2 regularizer\n",
    "_sampler_precision_multiplier = 1000\n",
    "_samples_num = 50\n",
    "\n",
    "encoder, decoder, model = get_model()\n",
    "cross_entropy = BCEHessianCalculator(wrt=\"weight\", shape=\"diagonal\", speed=\"half\")\n",
    "sampler = DiagLaplace()\n",
    "\n",
    "# initialize gaussian on the weight as isotropic N(0,1)\n",
    "mean = parameters_to_vector(model.parameters())\n",
    "precision = _prior_prec * torch.ones_like(mean)\n",
    "\n",
    "# train\n",
    "losses, losses_test, priors = [], [], []\n",
    "with torch.no_grad():\n",
    "    for epoch in tqdm(range(_epoch_num)):\n",
    "        # (Mean Squared Error + l2 reg)\n",
    "        # loss = \n",
    "        #  = - log p(y,theta) \n",
    "        #  = - log p(y|theta) - log p(theta)\n",
    "        #  = - log_gaussian - log_prior\n",
    "\n",
    "        # 0-order\n",
    "        log_bernoulli = cross_entropy.compute_loss(imgs, imgs, model).detach().numpy()\n",
    "        log_bernoulli_test = cross_entropy.compute_loss(imgs_test, imgs_test, model).detach().numpy()\n",
    "        log_prior = 0.5 * _prior_prec * torch.sum(mean**2).detach().numpy()\n",
    "        losses.append(log_bernoulli + log_prior); losses_test.append(log_bernoulli_test + log_prior)\n",
    "        priors.append(log_prior)\n",
    "\n",
    "        # 1-order\n",
    "        gradient_avg = torch.zeros_like(mean)\n",
    "        std_deviation = 1.0 / (precision * _sampler_precision_multiplier).sqrt()\n",
    "        samples = sampler.sample(mean, std_deviation, n_samples=_samples_num)\n",
    "        for parameter in samples: # montecarlo approximate the expected value\n",
    "            vector_to_parameters(parameter, model.parameters())\n",
    "            gradient_log_bernoulli = cross_entropy.compute_gradient(imgs, imgs, model) \n",
    "            gradient_log_prior = _prior_prec * parameter\n",
    "            gradient = gradient_log_bernoulli + gradient_log_prior\n",
    "            gradient_avg += gradient\n",
    "        gradient_avg /= _samples_num\n",
    "\n",
    "        # 2-order\n",
    "        vector_to_parameters(mean, model.parameters())\n",
    "        hessian = cross_entropy.compute_hessian(imgs, model)\n",
    "\n",
    "        # update gaussian on weights\n",
    "        mean -= _learning_rate * gradient_avg # gradient step\n",
    "        precision = (1-_memory_factor) * precision + hessian # laplace step\n",
    "\n",
    "        vector_to_parameters(mean, model.parameters())\n",
    "\n",
    "plot_training(losses, losses_test, priors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at posterior on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation = 1.0 / (precision * _sampler_precision_multiplier).sqrt()\n",
    "\n",
    "plot_std(std_deviation, encoder_size, decoder_size)\n",
    "plot_attention(std_deviation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fancy_latent(encoder, imgs)\n",
    "\n",
    "encoder_mean = mean[:encoder_size]\n",
    "encoder_std = std_deviation[:encoder_size]\n",
    "plot_latent(encoder, imgs, \n",
    "            posterior=(encoder_mean, encoder_std),\n",
    "            scale_radius=1,\n",
    "            n_sample=1000)\n",
    "\n",
    "for data_idx in range(10): #only show the first 10 images\n",
    "    plot_reconstruction_with_latent(model, imgs, \n",
    "                        posterior=(mean, std_deviation), \n",
    "                        data_idx=data_idx,\n",
    "                        scale_radius=1, n_sample=1000,\n",
    "                        apply_softmax=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_mean = mean[:encoder_size]\n",
    "encoder_std = std_deviation[:encoder_size]\n",
    "\n",
    "plot_latent(encoder, imgs_test, \n",
    "            posterior=(encoder_mean, encoder_std),\n",
    "            scale_radius=1,\n",
    "            n_sample=1000)\n",
    "plot_fancy_latent(encoder, imgs_test)\n",
    "\n",
    "for data_idx in range(10): #only show the first 10 images\n",
    "    plot_reconstruction_with_latent(model, imgs_test, \n",
    "                        posterior=(mean, std_deviation), \n",
    "                        data_idx=data_idx,\n",
    "                        scale_radius=1, n_sample=1000,\n",
    "                        apply_softmax=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample only the encoder (fix the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_deviation[-decoder_size:] = torch.zeros(decoder_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_idx in range(5): #only show the first 5 images\n",
    "    plot_reconstruction_with_latent(model, imgs, \n",
    "                        posterior=(mean, std_deviation), \n",
    "                        data_idx=data_idx,\n",
    "                        scale_radius=1, n_sample=1000,\n",
    "                        apply_softmax=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_idx in range(5): #only show the first 5 images\n",
    "    plot_reconstruction_with_latent(model, imgs_test, \n",
    "                        posterior=(mean, std_deviation), \n",
    "                        data_idx=data_idx,\n",
    "                        scale_radius=1, n_sample=1000,\n",
    "                        apply_softmax=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gauss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6c099567d37d7e8a434c28d3f0abbe2f74c3f11df2455f666970f25a5926fa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
